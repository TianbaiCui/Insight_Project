{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import gc\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import unicodecsv as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_date(deal_end, description):\n",
    "    end_day = ''\n",
    "    end_month = ''\n",
    "    if deal_end not in description:\n",
    "        return None\n",
    "    else:\n",
    "        i = 0\n",
    "        while description[description.find(deal_end)+len(deal_end)+i].isdigit():\n",
    "            end_month += description[description.find(deal_end)+len(deal_end)+i]\n",
    "            i += 1\n",
    "        i += 1\n",
    "        while description[description.find(deal_end)+len(deal_end)+i].isdigit():\n",
    "            end_day += description[description.find(deal_end)+len(deal_end)+i]\n",
    "            i += 1\n",
    "            if description.find(deal_end)+len(deal_end)+i == len(description):\n",
    "                break\n",
    "        return end_month, end_day   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling(url, brand, page_max=100):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "\n",
    "    master = [['Brand', 'Title', 'Description', 'Posted_date', 'End_date', 'Comments_count', 'Bookmarks_count', 'Shares_count']]\n",
    "    page = 0\n",
    "    while True:\n",
    "        \n",
    "        check_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            try:\n",
    "                WebDriverWait(driver, 3).until(lambda driver: driver.execute_script(\"return document.body.scrollHeight;\")  > check_height)\n",
    "                check_height = driver.execute_script(\"return document.body.scrollHeight;\") \n",
    "            except:\n",
    "                 break\n",
    "\n",
    "        print('Started')\n",
    "        elements = driver.find_elements_by_class_name('mlist')\n",
    "\n",
    "        for element in elements:\n",
    "            temp = [brand]\n",
    "            deal_id = element.get_attribute('id')  #first find the deal_id\n",
    "                \n",
    "            #------ find title of deal\n",
    "            try:\n",
    "                #title = element.find_element_by_xpath(\"//*[@id=\"+deal_id+\"]/div[2]/h2/a/span[1]\").text\n",
    "                #title = title + ' ' + element.find_element_by_xpath(\"//*[@id=\"+deal_id+\"]/div[2]/h2/a/span[2]\").text\n",
    "                title = element.find_element_by_class_name(\"indextitle\").text\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            temp.append(title)\n",
    "            \n",
    "            #------ find description of deal\n",
    "            #des = element.find_element_by_xpath('//*[@id='+ '\"' +deal_id+ '\"' + ']/div[2]/div[1]/table/tbody/tr/td/div/ul').text\n",
    "            des = element.find_element_by_tag_name('table').text\n",
    "            \n",
    "            for deal_ends in ['Deal ends on ', 'Deal ends ', 'Deal expires ', 'Coupon expires ']:\n",
    "                if not find_end_date(deal_ends, des):\n",
    "                    end_month = None\n",
    "                    end_day = None\n",
    "                    continue\n",
    "                else:\n",
    "                    end_month, end_day = find_end_date(deal_ends, des)\n",
    "                    try:\n",
    "                        end_month = int(end_month)\n",
    "                        end_day = int(end_day)\n",
    "                        if end_month > 12 or end_day > 31:\n",
    "                            end_month = None\n",
    "                            end_day = None\n",
    "                    except:\n",
    "                        pass\n",
    "                    break\n",
    "            \n",
    "            temp.append(des)\n",
    "            \n",
    "            #------ find time the deal was posted\n",
    "            try:\n",
    "                time = element.find_element_by_class_name('pubtime').text\n",
    "                time = time[0:-4]  #strip away 'Posted' and 'ago'\n",
    "                post_date = datetime.today()\n",
    "                if time[-4:] == 'days':\n",
    "                    post_date -= timedelta(days=int(time[0:-5]))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print ('===TIME NOT FOUND')\n",
    "                print ('===deal skipped')\n",
    "                continue\n",
    "            \n",
    "            temp.append(post_date.strftime(\"%m/%d/%Y\"))       \n",
    "            \n",
    "            #------ find time the deal was ended\n",
    "            \n",
    "            if end_month and end_day:\n",
    "                end_date = datetime(post_date.year, end_month, end_day)\n",
    "                end_date_saved = end_date.strftime(\"%m/%d/%Y\")\n",
    "            else:\n",
    "                end_date_saved = ''\n",
    "            \n",
    "                \n",
    "            temp.append(end_date_saved)\n",
    "            \n",
    "            stats = element.find_element_by_class_name(\"stat-count\")\n",
    "            stats_nums = stats.find_elements_by_class_name(\"j-count\")\n",
    "            \n",
    "            #------ find number of comments for the deal\n",
    "            num_comments = 0\n",
    "            try:\n",
    "                #path = '//*[@id=' + '\"' + deal_id + '\"'+ ']/div[2]/div[3]/div/t[1]/span'\n",
    "                #num_comments = element.find_element_by_xpath(path).text\n",
    "                num_comments = stats_nums[0].text\n",
    "                #print(num_comments)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            temp.append(num_comments)\n",
    "            \n",
    "            #------ find number of bookmarks for the deal\n",
    "            num_bookmarks = 0\n",
    "            try:\n",
    "                #path = '//*[@id=' + '\"' + deal_id + '\"' + ']/div[2]/div[3]/div/t[2]/span'\n",
    "                #num_bookmarks = element.find_element_by_xpath(path).text\n",
    "                num_bookmarks = stats_nums[1].text\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            temp.append(num_bookmarks)\n",
    "            \n",
    "            #------ find number of shares for the deal\n",
    "            num_shares = 0\n",
    "            try:\n",
    "                #path = '//*[@id=' + '\"' + deal_id + '\"'+ ']/div[2]/div[3]/div/t[3]/span'\n",
    "                #num_shares = element.find_element_by_xpath(path).text\n",
    "                num_shares = stats_nums[2].text\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            temp.append(num_shares)\n",
    "            \n",
    "            #------ append to master list\n",
    "            master.append(temp)\n",
    "\n",
    "        try:\n",
    "            #load = driver.find_element_by_xpath(\"//a[@title='Next']\")\n",
    "            load = driver.find_element_by_class_name(\"next_link\")\n",
    "            page += 4\n",
    "            print(\"Finished {} pages\".format(page))\n",
    "\n",
    "            # See if the last page has been reached\n",
    "            page_num = driver.find_element_by_class_name('pages').find_element_by_class_name('current').text\n",
    "            \n",
    "            if page_num == str(page_max):\n",
    "                print ('Last page reached')\n",
    "                break\n",
    "            else:\n",
    "                load.click()          \n",
    "        except:\n",
    "            print (\"===Can't go to the next page\")\n",
    "            break \n",
    "            \n",
    "    return master\n",
    "\n",
    "def saveCSV(filename, data):\n",
    "    with open(filename, 'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.dealmoon.com/Online-Stores/Nike-Store\"\n",
    "data = crawling(url, 'Nike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCSV('Nike_store_deals.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for mac64 chromedriver:75.0.3770.8 in cache\n",
      "Driver found in /Users/tianbai/.wdm/chromedriver/75.0.3770.8/mac64/chromedriver\n",
      "Started\n",
      "Finished 4 pages\n",
      "Started\n",
      "Finished 8 pages\n",
      "Started\n",
      "Finished 12 pages\n",
      "Started\n",
      "Finished 16 pages\n",
      "Started\n",
      "Finished 20 pages\n",
      "Started\n",
      "Finished 24 pages\n",
      "Started\n",
      "Finished 28 pages\n",
      "Started\n",
      "Finished 32 pages\n",
      "Started\n",
      "Finished 36 pages\n",
      "Started\n",
      "Finished 40 pages\n",
      "Started\n",
      "===Can't go to the next page\n"
     ]
    }
   ],
   "source": [
    "url_adidas = 'https://www.dealmoon.com/Online-Stores/adidas-com'\n",
    "data = crawling(url_adidas, 'Adidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCSV('Adidas_deals.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for mac64 chromedriver:75.0.3770.90 in cache\n",
      "Driver found in /Users/tianbai/.wdm/chromedriver/75.0.3770.90/mac64/chromedriver\n",
      "Started\n",
      "Finished 4 pages\n",
      "Started\n",
      "Finished 8 pages\n",
      "Started\n",
      "Finished 12 pages\n",
      "Started\n",
      "Finished 16 pages\n",
      "Started\n",
      "Finished 20 pages\n",
      "Started\n",
      "Finished 24 pages\n",
      "Started\n",
      "Finished 28 pages\n",
      "Started\n",
      "Finished 32 pages\n",
      "Started\n",
      "Finished 36 pages\n",
      "Started\n",
      "Finished 40 pages\n",
      "Started\n",
      "Finished 44 pages\n",
      "Started\n",
      "Finished 48 pages\n",
      "Started\n",
      "Finished 52 pages\n",
      "Started\n",
      "Finished 56 pages\n",
      "Started\n",
      "Finished 60 pages\n",
      "Started\n",
      "Finished 64 pages\n",
      "Started\n",
      "Finished 68 pages\n",
      "Started\n",
      "Finished 72 pages\n",
      "Started\n",
      "Finished 76 pages\n",
      "Started\n",
      "Finished 80 pages\n",
      "Started\n",
      "===Can't go to the next page\n"
     ]
    }
   ],
   "source": [
    "url_ToryBurch = 'https://www.dealmoon.com/Online-Stores/ToryBurch'\n",
    "data = crawling(url_ToryBurch, 'ToryBurch')\n",
    "saveCSV('ToryBurch.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.dealmoon.com/Online-Stores/ToryBurch'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_ToryBurch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Reebok = 'https://www.dealmoon.com/Online-Stores/Reebok'\n",
    "data = crawling(url_Reebok, 'Reebok')\n",
    "saveCSV('Reebok.csv', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
